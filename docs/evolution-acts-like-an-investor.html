<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Emerging a Society with MARL Part 2: Evolution Acts Like an Investor</title>
    <link rel="icon" href="static/icon.jpg" type="image/jpg" />
    <link rel="stylesheet" href="static/style.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"
      integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP"
      crossorigin="anonymous"
    />
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js"
      crossorigin="anonymous"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/mathtex-script-type.min.js"
      crossorigin="anonymous"
    ></script>
    <meta
      name="description"
      content="Maximizing expected wealth leads investors to bankruptcy. Just as maximizing expected genes copies leads species to extinction. Investors found a neat solution to this problem, our results show that the same trick works for Evolution."
    />
    <meta name="author" content="Jo√£o Abrantes" />

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Emerging a Society with MARL Part 2: Evolution Acts Like an Investor" />
    <meta
      name="twitter:description"
      content="Maximizing expected wealth leads investors to bankruptcy. Just as maximizing expected genes copies leads species to extinction. Investors found a neat solution to this problem, our results show that the same trick works for Evolution."
    />
    <meta
      name="twitter:image"
      content="static/icon.jpg"
    />
    <meta name="twitter:site" content="@joaoabrantis" />
  </head>
  <body>
    <header>
      <button onclick="location.href='index.html'">All Articles</button>
      <button class="theme-toggle" id="theme-toggle" aria-label="switch theme">
        üåô/‚òÄÔ∏è
      </button>
    </header>
    <main>
<article>
    <div class="post-header">
        <h1>Emerging a Society with MARL Part 2</h1>
        
            <p class="subtitle">Evolution Acts Like an Investor</p>
        
    </div>
    <p><em>Published on 2025-10-24</em></p>
    <style>

p {
 margin-block-end: 0em !important;
}
latexcenter:first-child{
  font: normal 2em KaTeX_Main,Times New Roman,serif;
  line-height: 1.5;
}
</style>

<p>In <a href="/the-key-properties-to-reproduce-society-with-marl.html">Part 1</a> of this series, I proposed four key properties that an environment must have for societies to emerge, and introduced <strong>Territories</strong>, a multi-agent world built around those principles. Now that the right environment is in place, the next step is to apply the right pressure on its agents to foster the emergence of social intelligence.</p>
<p>Nature drives evolutionary pressure for agents to develop ever greater capabilities to maximise the survival and reproduction of their genes. We want to maximise the same objective but with a more efficient optimiser: <strong>Reinforcement Learning</strong> instead of Evolution.</p>
<p>When we trained agents to maximise their number of gene copies, they overpopulated the world and drove themselves extinct. We later noticed that investors face the same underlying problem: na√Øve wealth maximisation leads to bankruptcy. Investors discovered a simple mathematical trick to avoid it ‚Äî and our results show that the same principle also works for evolution.</p>
<p>Our earlier paper [1] introduced a reward function to mimic evolutionary pressure. At the end, we hinted at a better alternative for future work. In this Part, I finally test that idea and describe both the theory and results of why this novel reward function best captures evolutionary pressure. But before we can design rewards that maximize gene copies, we need to understand how genes are represented and inherited in <em>Territories</em>.</p>
<h2 id="how-dna-works-in-territories">How DNA Works in Territories</h2>
<p>In the default settings, each agent carries 3 genes, and each gene can have one of four values (called alleles, integers from 0 to 3). The policy of each agent is conditional on their DNA.</p>
<p>We create 32-dimensional embeddings for each of the 12 possible alleles (3 genes √ó 4 alleles per gene). Each agent's allele maps to its corresponding embeddings, then we average all the embeddings from the three genes to produce the policy input. This design allows different genes to cause different behaviors while ensuring that agents with overlapping genes share some behavioral features. Think of the number of dimensions in the allele embeddings as degrees of freedom to change the policy according to the DNA.</p>
<p><img src="static/img/kinship_marl_part_2/genetics.png" alt="meme" style="width:85%;margin:auto;display:block;"/></p>
<p>The agents can see their relative kinship with other visible agents and observe their DNA as one-hot encoded vectors. This allows agents to distinguish between potentially cooperative and non-cooperative policies, regardless of kinship. In our visualization, we represent the first gene with different skin colors, the second with different hair colors, and the third with different outfits.</p>
<p>When two agents reproduce, each gene of the child is randomly selected from one of the parents. These genetic parameters (number of genes, alleles per gene) are configurable and will be adjusted for different experiments.</p>
<h2 id="the-evolutionary-reward-function">The Evolutionary Reward Function</h2>
<p>The challenge of maximizing gene copies shares a surprising parallel with wealth maximization in finance. In both cases, outcomes compound over time; more money generates additional money, just as more relatives carrying your genes generate more gene copies. Critically, both require avoiding catastrophic failure: bankruptcy for investors, extinction for species.</p>
<h3 id="how-to-maximise-wealth-with-the-kelly-criterion">How to Maximise Wealth with the Kelly Criterion</h3>
<p>Consider an investor facing a sequence of binary bets with probability p of doubling their stake and probability 1-p of losing it entirely. A naive objective would be to maximize the expected wealth after N bets. If p &gt; 0.5, this objective is achieved by the investor betting its entire wealth at every bet:</p>
<p>
<script type="math/tex; mode=display">\mathbb{E}[W_N] = W_0(2p)^N</script>
</p>
<p>However, this strategy leads almost certainly to ruin:</p>
<p>
<script type="math/tex; mode=display">\text{risk of bankruptcy} = 1 - p^N</script>
</p>
<p>For any long horizon, this risk approaches 100%. The problem is that expected value doesn‚Äôt represent typical outcomes; it‚Äôs dominated by an extremely rare event where all bets succeed, while the median outcome is bankruptcy.</p>
<p>The solution, well-established in finance, is to maximize the expected logarithm of wealth instead. This formulation prioritises the avoidance of bankruptcy above everything else since log(0) = -‚àû. As Warren Buffett says, the first rule of investing is: <strong>don‚Äôt lose money</strong>, and the second rule is to never forget the first rule. Only once you have strategies that can safely avoid bankruptcy, you can start maximising growth.</p>
<p>Maximising the logarithm of wealth gives rise to the Kelly Criterion ‚Äî a provably optimal strategy for long-term wealth growth in the super simple scenario of sequential binary bets with known odds. In practice, we approximate the infinite penalty with a very large negative value to avoid breaking the learning algorithm.</p>
<h3 id="from-wealth-to-gene-copies">From Wealth to Gene Copies</h3>
<p>Evolution operates on the same principle. Our agents seek to maximize copies of their genes across the population. We define gene copy count at time <script type="math/tex">t</script> for agent <script type="math/tex">i</script> as:</p>
<p>
<script type="math/tex; mode=display">c_t^i=\sum_{j \in \mathbb{A}_t}{k(i,j)}</script>
</p>
<p>where <script type="math/tex">k(i,j)</script> represents the kinship (number of shared genes) between agents <script type="math/tex">i</script> and <script type="math/tex">j</script>, summed over all living agents at time <script type="math/tex">t</script>.</p>
<p>The most straightforward evolutionary reward would assign 0 everywhere except <script type="math/tex">c^i_N</script> (or <script type="math/tex">\log(c^i_N)</script>) at episode termination. But this creates two critical problems:</p>
<ol>
<li><strong>Sparse rewards</strong> provide minimal learning signal during training</li>
<li><strong>Finite episodes</strong> conflict with our goal of infinite-horizon learning</li>
</ol>
<h3 id="from-terminal-to-dense-rewards">From Terminal to Dense Rewards</h3>
<p>We can transform these terminal rewards into dense, infinite-horizon compatible rewards by rewriting them as sums of step-by-step changes:</p>
<p><strong>Linear reward (maximizing estimated gene copies):</strong></p>
<p>
<script type="math/tex; mode=display">
\begin{align*}
c_N^i &= c_0^i + \sum_{t=1}^N \big(c_t^i - c_{t-1}^i\big), \\[6pt]
\max_{\pi}~\mathbb{E}\!\big[c_N^i\big]
&= \max_{\pi}~\mathbb{E}\!\Big[\sum_{t=1}^N \big(c_t^i - c_{t-1}^i\big)\Big]
= \max_{\pi}~\mathbb{E}\!\Big[\sum_{t=1}^N r_t^i\Big], \\[6pt]
\text{where}\quad r_t^i &= c_t^i - c_{t-1}^i.
\end{align*}
</script>
</p>
<p><strong>Logarithmic reward (Kelly-inspired):</strong></p>
<p>
<script type="math/tex; mode=display">
\begin{align*}
c_N^i &= c_0^i \prod_{t=1}^N \frac{c_t^i}{c_{t-1}^i}, \\[6pt]
\max_{\pi}~\mathbb{E}\!\big[\log(c_N^i)\big]
&= \max_{\pi}~\mathbb{E}\!\Big[\sum_{t=1}^N \log\!\left(\frac{c_t^i}{c_{t-1}^i}\right)\Big]
= \max_{\pi}~\mathbb{E}\!\Big[\sum_{t=1}^N r_t^i\Big], \\[6pt]
\text{where}\quad r_t^i &= \log\!\left(\frac{c_t^i}{c_{t-1}^i}\right).
\end{align*}
</script>
</p>
<p>Both formulations provide dense learning signals and work even when <script type="math/tex">N</script> goes to infinity.</p>
<p>The logarithm creates loss aversion ‚Äî it penalizes losses more than it rewards equivalent gains. With 4 gene copies in the gene pool:</p>
<ul>
<li>Losing 1 copy (4 ‚Üí 3): <script type="math/tex">\log(\frac{3}{4}) \approx -0.288</script>
</li>
<li>Gaining 1 copy (4 ‚Üí 5): <script type="math/tex">\log(\frac{5}{4}) \approx +0.223</script>
</li>
</ul>
<p>A 50-50 gamble between these outcomes yields negative expected reward (-0.032), even though the expected gene change is neutral. This asymmetry scales: a 25% loss always hurts more than a 25% gain helps.</p>
<p>The Kelly-inspired reward is theoretically optimal for sequential binary bets with known odds. However, we need to test how it behaves for gene copy maximisation in the complex world of <em>Territories</em>.</p>
<p>We'll do this just after the next section where we explore the <strong>family value function</strong>.</p>
<h2 id="the-family-value-function">The Family Value Function</h2>
<p>Each agent estimates the value of a given state using only its local observation. However, the agent is rewarded according to what happens to its entire family! If this were the whole story, we‚Äôd be in deep trouble.</p>
<p>Imagine an agent that‚Äôs done everything right. Around it lies abundance: stored food, summer time and cooperative neighbors. It sees a bright future for its family, and therefore estimates a high value. Yet, in some distant place beyond its vision, its relatives are starving and dying. The agent begins receiving less reward than expected, assumes it's at fault, and mistakenly adjusts its policy!</p>
<p>Taking the blame (or credit) for outcomes unrelated to one‚Äôs actions is a terrible way to learn. To solve this the agent needs to either 1) observe the <strong>global</strong> state of its family, 2) intelligently aggregate the local value estimates of all its family members. This is the classic credit assignment problem in multi-agent reinforcement learning (MARL).</p>
<p><strong>The traditional solution</strong>: In team-based MARL (e.g., Dota [2] or Starcraft [3]), each team uses one large neural network that observes all teammates. This network has separate heads for the policy and value function. A centralized value function that sees everything is the easiest way to solve multi-agent credit assignment.</p>
<p>This is completely legitimate! The value function only helps us optimize ‚Äî we want the best value function possible to efficiently train evolutionarily fit policies. The <em>policy</em>, on the other hand, must be realistic: we use policies to test hypotheses about real agents with limited knowledge.</p>
<p><strong>Why this doesn't work for kinship-aligned MARL</strong>:</p>
<ol>
<li>
<p><strong>Policies can't receive observations from other agents</strong>. In kinship-aligned MARL each agent can potentially compete with any other agent. For example, in the limit when there are only enough resources for one agent to survive everyone is competing against each other. We don‚Äôt want an agent‚Äôs policy receiving observations from potential competitors.</p>
</li>
<li>
<p><strong>Each agent has a unique value function</strong>. Each agent optimizes for their unique genes, not a shared team reward. Therefore, each needs their own value function.</p>
</li>
</ol>
<p>We could still build a centralized value function with one output per agent, but this would be computationally expensive ‚Äî especially since we couldn't share weights between the value network and policy network.</p>
<p><strong>The solution: Evolutionary Value-Decomposition Network (E-VDN)</strong> [1].</p>
<p>Thankfully, there's a better approach. Each agent's value function is unique, but it's correlated with relatives' value functions proportionally to their kinship. We can estimate the <strong>family value function</strong> (<script type="math/tex">V</script>) by aggregating the local (and noisy) value (<script type="math/tex">\tilde{V}</script>) estimates of each agent in the following way:</p>
<p>
<script type="math/tex; mode=display">
V^i(s^1, \ldots, s^{|\mathbb{A}_t|}) \approx \frac{1}{c_t^i}\sum_{j \in \mathbb{A}_t}{k(i, j)\overset{\sim}{V}^j(s^j_t)}
</script>
</p>
<p><strong>Training</strong>: We train each individual value <script type="math/tex">\tilde{V}</script> so that the family value function satisfies the Bellman equation.</p>
<p>
<script type="math/tex; mode=display">
V^i(s) = \mathbb{E}_{a \sim \pi, s' \sim P(\cdot|s,a)}\left[r(s, a, s') + \gamma V^i(s')\right]
</script>
</p>
<p><strong>Value beyond death:</strong> If agent <script type="math/tex">i</script> dies transitioning from state <script type="math/tex">s</script> to <script type="math/tex">s'</script>, we still estimate its family value function for the next state using its remaining relatives. Only when an entire family goes extinct, we set its family value function to zero. This incentivizes agents to care about their genes' future beyond their own lifetime.</p>
<p>In simple terms, the agent is incentivised to:</p>
<ol>
<li><strong>Survive and replicate</strong></li>
<li><strong>Help its family</strong> survive and replicate</li>
<li><strong>Ensure that when it dies, its family is in a good position to carry on</strong> surviving and replicating</li>
</ol>
<p>In our previous work [1], we showed this works empirically: old and infertile agents learned to sacrifice themselves for the good of their family.</p>
<p><strong>Toy example</strong>: To illustrate how E-VDN reduces estimation error compared to individual noisy estimates, consider this simple numerical example:</p>
<div class="highlight"><pre><span></span><code><span class="n">num_agents</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">num_genes</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_alleles</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">num_future_agents</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># The DNA for each agent in current step (num_agents, num_genes)</span>
<span class="n">dnas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_alleles</span><span class="p">,</span> <span class="p">(</span><span class="n">num_agents</span><span class="p">,</span> <span class="n">num_genes</span><span class="p">))</span>

<span class="c1"># The normalized kinship matrix (num_agents, num_agents)</span>
<span class="c1"># Note k(i, i) = 1. k(i, j) = k(j, i)</span>
<span class="n">kinship_matrix</span> <span class="o">=</span> <span class="p">(</span><span class="n">dnas</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">dnas</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_genes</span>
<span class="c1"># The number of gene copies each agent has in the gene pool (num_agents,)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">kinship_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_genes</span>

<span class="c1"># Reproduce some agents asexually to simulate the future population</span>
<span class="n">reproduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">num_agents</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_future_agents</span><span class="p">)</span>
<span class="n">future_dnas</span> <span class="o">=</span> <span class="n">dnas</span><span class="p">[</span><span class="n">reproduced</span><span class="p">]</span>  <span class="c1"># shape (num_future_agents, n_genes)</span>

<span class="c1"># The number of gene copies each CURRENT agent will have in the FUTURE</span>
<span class="c1"># (num_agents,)</span>
<span class="n">c_future</span> <span class="o">=</span> <span class="p">(</span><span class="n">dnas</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">future_dnas</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># The growth delta c_future - c</span>
<span class="c1"># This is the ground truth, an ideal value function would predict this value</span>
<span class="n">delta_c</span> <span class="o">=</span> <span class="n">c_future</span> <span class="o">-</span> <span class="n">c</span> <span class="c1"># (num_agents,)</span>

<span class="c1"># We simulate noisy estimates of delta_c as the local value estimate of each agent.</span>
<span class="c1"># The noise is due to their lack of global information.</span>
<span class="n">local_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">delta_c</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">delta_c</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Make the kinship-weighted aggregation (aka E-VDN)</span>
<span class="n">e_vdn_estimate</span> <span class="o">=</span> <span class="n">kinship_matrix</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">local_values</span><span class="p">)</span> <span class="o">/</span> <span class="n">kinship_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">local_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">local_values</span> <span class="o">-</span> <span class="n">delta_c</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">e_vdn_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">e_vdn_estimate</span> <span class="o">-</span> <span class="n">delta_c</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Local MeanSquaredError: </span><span class="si">{</span><span class="n">local_mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;E-VDN MeanSquaredError: </span><span class="si">{</span><span class="n">e_vdn_mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">Local</span> <span class="n">MeanSquaredError</span><span class="p">:</span> <span class="mf">269.9550627937862</span>
<span class="o">&gt;</span> <span class="n">E</span><span class="o">-</span><span class="n">VDN</span> <span class="n">MeanSquaredError</span><span class="p">:</span> <span class="mf">47.37662428672761</span>
</code></pre></div>

<p>The toy example shows how kinship-weighted aggregation substantially reduces estimation error. With this value function problem solved, we can train agents to maximize evolutionary fitness!</p>
<p><strong>A further improvement: Gene-Centric Value Aggregation (GCVA)</strong></p>
<p>While writing this post, I realized E-VDN could be improved further. Instead of aggregating from each agent's perspective, we can aggregate from each gene's perspective ‚Äî estimating the value function for each allele separately, then combining them. I call this Gene-Centric Value Aggregation (GCVA).</p>
<p>Since both approaches solve the core credit assignment problem, I've moved the GCVA details to the appendix. So now, we can go straight to the experiments!</p>
<h2 id="experiment-setup-and-results">Experiment Setup and Results</h2>
<p><strong>Environment configuration:</strong></p>
<ul>
<li>Episodes begin just before the optimal harvest season</li>
<li>4 pairs of agents, where agents within each pair share identical DNA and spawn close together (to facilitate reproduction)</li>
<li>Episodes reset around step ~500, but agents experience them as infinite-horizon (we never zero the value function at episode boundaries). This reset strategy ensures agents see the initial conditions multiple times.</li>
<li>Environment code available <a href="https://github.com/jpiabrantes/territories/tree/2nd_blogpost">here</a>.</li>
</ul>
<p><strong>Policy architecture:</strong></p>
<p>Our policy is similar to the one used in Neural MMO [4]: convolutional layers for visual features, dense layers for scalars, and an LSTM for temporal integration (code <a href="https://github.com/PufferAI/PufferLib/blob/2755f6331a12028cb3b2774a16f5253b32b4d228/pufferlib/ocean/torch.py#L59">here</a>).</p>
<p><strong>Experimental conditions:</strong>
The figure below shows four training runs comparing:</p>
<ul>
<li><strong>Reward functions:</strong> Naive growth delta (blue) vs. Kelly-inspired logarithmic reward (green)</li>
<li><strong>Value aggregation:</strong> E-VDN (solid lines) vs. GCVA (dashed lines)</li>
</ul>
<p><img src="static/img/kinship_marl_part_2/results.jpeg" alt="results" style="width:100%;margin:auto;display:block;"/></p>
<p><strong>Results:</strong> When maximizing the naive reward (blue lines), agents reproduce too aggressively ‚Äî birth rates and peak populations spike early in training. Their large families quickly deplete food stores, leading to extinction before the second harvest season arrives. As a result, episode length struggles to exceed 200 steps.</p>
<p>In contrast the Kelly-inspired reward (green lines) first focuses agents on learning how to survive. The graphs show that only after the episode length reaches its maximum the agents start to increase the amount of food they store to sustain larger populations. This approach successfully teach the agents how to grow their population in a safe way that avoids extinction.</p>
<p>We don‚Äôt see any major difference between E-VDN and GCVA. Although it does seem like GCVA slightly increases the reward obtained in both cases: 1) with the naive reward it achieves a slightly higher birth rate and peak population, and in 2) with the Kelly reward life expectancy is increased at the expense of a lower peak population. It's encouraging to see that both algorithms work well, but to distinguish their performance we would need a more careful experiment and multiple independent training runs.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this experiment, <strong>population control was forced to emerge</strong> because there were <strong>no hardcoded property rights</strong>. The food had to be stored in a physical location and could be exhausted by other agents if the population grew too large. In our old environment [1], the naive reward worked fine because resources gathered by one agent were <em>magically owned</em> by that agent. This contrast is encouraging as it strengthens our hypothesis that <strong><em>Territories</em> can lead to the emergence of more interesting social behaviours</strong>.</p>
<p>Like anything that compounds, <strong>the best strategy for growing gene copies is to avoid loss first, then pursue safe growth</strong>.</p>
<p>The Warren Buffett of Evolution would say: Rule #1 is don't lose family members. Rule #2 is never forget Rule #1.</p>
<hr />
<h2 id="citing-this-work">Citing this work</h2>
<p>If you use the Kelly-inspired evolutionary reward or the Gene-Centric Value Aggregation (GCVA) method in your research, please cite:</p>
<div class="highlight"><pre><span></span><code>@misc{abrantes2025evolution_investor,
   author = {Abrantes, Jo√£o P.},
   title = {Emerging a Society with MARL Part 2: Evolution Acts Like an Investor},
   year = {2025},
   month = {October},
   url = {https://abranti.com/evolution-acts-like-an-investor.html},
   note = {Blog post}
}
</code></pre></div>

<p>For the original E-VDN method and kinship-aligned MARL framework, please also cite our earlier work [1].</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>I would like to thank Arnaldo Abrantes for reading and improving an early version of this post.</p>
<p>All errors, omissions and imprecisions remain my own.</p>
<h2 id="appendix-gene-centred-value-aggregation">Appendix: Gene-Centred Value Aggregation</h2>
<p>GCVA works in the following way:</p>
<ol>
<li>Each agent estimates a local value function that corresponds to the expected increase of its gene copies in the gene pool (or to the log of its growth rate).</li>
<li>This expected growth is mapped to each gene of the agent. Since growth compounds, each agent believes that genes that are shared with more agents will proportionally lead to higher changes in total gene counts.</li>
<li>In this way, every agent has estimated the growth of each of its genes ‚Äî we then average all these estimates to get a good estimate of the growth of each gene.</li>
<li>Finally we obtain the total growth of each agent gene by summing all of its corresponding gene growth estimates.</li>
</ol>
<p>We can adapt the toy example shown above to compute this:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># [... previous code here ...]</span>
<span class="c1"># How many agents carry each allele (num_genes, num_alleles)</span>
<span class="n">num_carriers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_genes</span><span class="p">,</span> <span class="n">num_alleles</span><span class="p">))</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_agents</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_genes</span><span class="p">):</span>
        <span class="n">num_carriers</span><span class="p">[</span><span class="n">g</span><span class="p">,</span> <span class="n">dnas</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">g</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Aggregate all the estimates for allele values</span>
<span class="n">allele_value_aggregation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_genes</span><span class="p">,</span> <span class="n">num_alleles</span><span class="p">))</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_agents</span><span class="p">):</span>
    <span class="n">c_a</span> <span class="o">=</span> <span class="n">kinship_matrix</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">num_genes</span>
    <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_genes</span><span class="p">):</span>
        <span class="n">allele_value_aggregation</span><span class="p">[</span><span class="n">g</span><span class="p">,</span> <span class="n">dnas</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">g</span><span class="p">]]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">num_carriers</span><span class="p">[</span><span class="n">g</span><span class="p">,</span> <span class="n">dnas</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">g</span><span class="p">]]</span> <span class="o">*</span> <span class="n">local_values</span><span class="p">[</span><span class="n">a</span><span class="p">])</span> <span class="o">/</span> <span class="n">c_a</span>
<span class="n">non_zero_carriers</span> <span class="o">=</span> <span class="n">num_carriers</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">allele_value_aggregation</span><span class="p">[</span><span class="n">non_zero_carriers</span><span class="p">]</span> <span class="o">/=</span> <span class="n">num_carriers</span><span class="p">[</span><span class="n">non_zero_carriers</span><span class="p">]</span>

<span class="c1"># Estimate the value for each agent by summing the gene values</span>
<span class="n">gcva_estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_agents</span><span class="p">)</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_agents</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_genes</span><span class="p">):</span>
        <span class="n">gcva_estimate</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">allele_value_aggregation</span><span class="p">[</span><span class="n">g</span><span class="p">,</span> <span class="n">dnas</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">g</span><span class="p">]]</span>

<span class="n">gcva_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">gcva_estimate</span> <span class="o">-</span> <span class="n">delta_c</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Local MeanSquaredError: </span><span class="si">{</span><span class="n">local_mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;E-VDN MeanSquaredError: </span><span class="si">{</span><span class="n">e_vdn_mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GCVA MeanSquaredError: </span><span class="si">{</span><span class="n">gcva_mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">Local</span> <span class="n">MeanSquaredError</span><span class="p">:</span> <span class="mf">269.9550627937862</span>
<span class="o">&gt;</span> <span class="n">E</span><span class="o">-</span><span class="n">VDN</span> <span class="n">MeanSquaredError</span><span class="p">:</span> <span class="mf">47.37662428672761</span>
<span class="o">&gt;</span> <span class="n">GCVA</span> <span class="n">MeanSquaredError</span><span class="p">:</span> <span class="mf">23.432493888625977</span>
</code></pre></div>

<p>Note that the way we generated new agents is important:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Reproduce some agents asexually to simulate the future population</span>
<span class="n">reproduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">num_agents</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_future_agents</span><span class="p">)</span>
<span class="n">future_dnas</span> <span class="o">=</span> <span class="n">dnas</span><span class="p">[</span><span class="n">reproduced</span><span class="p">]</span>  <span class="c1"># shape (num_future_agents, n_genes)</span>
</code></pre></div>

<p>We generate new agents from the DNAs of previous agents. Therefore, the assumption that gene copies compound is correct. If we had generated the <code>future_dnas</code> uniformly random, like this:</p>
<div class="highlight"><pre><span></span><code><span class="n">future_dnas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_alleles</span><span class="p">,</span> <span class="p">(</span><span class="n">num_future_agents</span><span class="p">,</span> <span class="n">num_genes</span><span class="p">))</span>
</code></pre></div>

<p>Then the previous assumption wouldn‚Äôt work, and we would need to adapt GCVA. However, in our environment new DNAs are always created from old DNAs.</p>
<p>Adapting this code to handle the logarithm of the growth rate, or estimating the value of dead agents is tricky but a fun exercise! I will eventually share the solution :)</p>
<h2 id="citations">Citations</h2>
<p>[1] Abrantes, Jo√£o P., Arnaldo J. Abrantes, and Frans A. Oliehoek. "Mimicking evolution with reinforcement learning." arXiv preprint arXiv:2004.00048 (2020).</p>
<p>[2] Berner, Christopher, et al. "Dota 2 with large scale deep reinforcement learning." arXiv preprint arXiv:1912.06680 (2019).</p>
<p>[3] Vinyals, Oriol, et al. "Grandmaster level in StarCraft II using multi-agent reinforcement learning." nature 575.7782 (2019): 350-354.</p>
<p>[4] Suarez, Joseph, et al. "Neural MMO 2.0: a massively multi-task addition to massively multi-agent learning." Advances in Neural Information Processing Systems 36 (2023): 50094-50104.</p>
</article>
</main>
    <footer>
      Tech Entrepreneur and Researcher into Complex Systems and Multi-Agent
      RL<br /><a href="https://x.com/joaoabrantis">@joaoabrantis on Twitter</a>
    </footer>
    <script>
      (() => {
        const root = document.documentElement;
        const btn = document.getElementById("theme-toggle");
        const store = localStorage;
        const DARK = "dark";
        const LIGHT = "light";

        /* 1 ‚ñ∏ restore saved choice, if any */
        const saved = store.getItem("theme");
        if (saved === DARK || saved === LIGHT) {
          root.setAttribute("data-theme", saved);
        }

        /* helper: is the page currently dark? */
        const isDark = () => {
          const attr = root.getAttribute("data-theme");
          return attr
            ? attr === DARK
            : window.matchMedia("(prefers-color-scheme: dark)").matches;
        };

        /* 2 ‚ñ∏ flip on click */
        btn?.addEventListener("click", () => {
          const next = isDark() ? LIGHT : DARK;
          root.setAttribute("data-theme", next);
          store.setItem("theme", next);
        });
      })();
    </script>
  </body>
</html>